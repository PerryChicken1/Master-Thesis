{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit for Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow in the footsteps of Gutiérrez et al (2017). Let us first formulate our objective mathematically.\n",
    "\n",
    "#### Modelling Objective\n",
    "\n",
    "We want to model $f: (x, \\mathbf{p}) \\mapsto y$, where $f$ belongs to the class of linear regression estimators parameterized by vector $\\mathbf{p}$. $x_i$  and $y_i$ are the RH98 and AGBD of footprint $i$, respectively. \n",
    "\n",
    "Parameters $\\mathbf{p}$ are estimated using a training set $S^T = \\{ s_1, \\ldots, s_{N_{\\text{train}}} \\}$ consisting of samples $s = (x,y)$. However, $S^T$ is merely a subsample from the available training data $S = \\{ h_1, \\ldots, h_{N_{\\text{total}}} \\} $ which consists of _hidden_ samples of the form $h = \\{ \\tilde{x}, \\tilde{y}, \\mathbf{m} \\}$. They are hidden because RH98 $\\tilde{x}$ and AGBD $\\tilde{y}$ are only revealed after the sample is added to the training set. By contrast, metadata $\\mathbf{m}$ about the sample is known à priori. To begin with, $S$ consists of all data in Ghana with PFT class = 2. \n",
    "\n",
    "#### Sample Efficiency Objective\n",
    "\n",
    "Unlike Gutiérrez et al, we do not incur a large cost from observing $\\tilde{x}$ or $\\tilde{y}$. However, we are interested in selecting only the most relevant samples according to $\\mathbf{m}$ all the same. We will later generalise our method to larger, more complicated datasets and more sophisticated model classes, hence sample efficiency will be important to minimise the computational burden of training the model.\n",
    "\n",
    "We select samples by partitioning $S$ into a pre-defined number of bins $\\eta_j$ for select $m_j$ $(1 \\leq j \\leq d)$ in $\\mathbf{m}$. For instance, if $m_j$ is a categorical variable with four categories, we create the j'th partition $S = \\bigcup_{k=1}^{4} C_k^j$, where $C_1^j$ contains all samples where $m_j$ is in class 1, $C_2^j$ contains all samples where $m_j$ is in class 2, etc. If $m_j$ is continuous, we quantize the variable into bins and partition accordingly. All the clusters generated using different meta information are then merged into a set of clusters $\\mathcal{C} = \\{C_l^j\\}$. Our hypothesis is that some clusters $C_i \\in \\mathcal{C}$ contain more relevant information for the modelling task than others, but we do not know which. This motivates us to train a multi-armed bandit who will simultaneously _explore_ the clusters to find out which contain the most relevant information and eventually _exploit_ these clusters to maximise the share of data therefrom.\n",
    "\n",
    "The multi-armed bandit algorithm is described in detail in their paper, but to summarise:\n",
    "\n",
    "* At every time $t$:\n",
    "    * Sample the probability of reward $\\hat{\\pi}_i$ from $Beta(\\alpha_i, \\beta_i)$ for every cluster\n",
    "    * Pick a datapoint $s$ from the cluster with highest probability of reward\n",
    "    * Add $s$ to $S^T$ and re-train the model\n",
    "    * Predict $\\hat{y}$ for a holdout validation set\n",
    "    * If loss decreases (increases), $r_t = 1 \\ (-1)$\n",
    "    * Update $\\alpha_i, \\beta_i$ based on $r_t$ \n",
    "\n",
    "#### Summary of Objective\n",
    "\n",
    "Our dual objectives are then:\n",
    "\n",
    "1) To optimize the performance of $f$ at predicting $y$, as measured by the MAPE on a holdout test set.\n",
    "2) To include as few samples as necessary in $S^T$.\n",
    "\n",
    "We will train a multi-armed bandit data selector to achieve both in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pyogrio\n",
    "import os\n",
    "import pandas.api.types as ptypes\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "os.chdir(r\"C:\\Users\\nial\\OneDrive\\ETH Zürich\\Master Thesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually set `dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "     'pft_class': 'category'\n",
    "    ,'region_cla': 'category'\n",
    "    ,'leaf_off_f': 'category'\n",
    "    ,'urban_prop': 'int64'\n",
    "    ,'agbd': 'float64'\n",
    "    ,'agbd_se': 'float64'\n",
    "    ,'beam': 'category'\n",
    "    ,'elev_lowes': 'float64'\n",
    "    ,'lat_lowest': 'float64'\n",
    "    ,'lon_lowest': 'float64'\n",
    "    ,'selected_a': 'category'\n",
    "    ,'shot_numbe': 'int64'\n",
    "    ,'sensitivit': 'float64'\n",
    "    ,'solar_elev': 'float64'\n",
    "    ,'rh98': 'float64'\n",
    "    ,'pattern': 'object'\n",
    "    ,'doy_sin': 'float64'\n",
    "    ,'doy_cos': 'float64'\n",
    "    ,'date': 'int64'\n",
    "    ,'lat_cos': 'float64'\n",
    "    ,'lat_sin': 'float64'\n",
    "    ,'lon_cos': 'float64'\n",
    "    ,'lon_sin': 'float64'\n",
    "    ,'pft_class_group': 'category'\n",
    "    ,'geometry': 'object'\n",
    "}\n",
    "\n",
    "df_ghana_subsample = pd.read_csv('df_ghana_subsample.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save `df_ghana`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nial\\anaconda3\\envs\\MasterThesis\\Lib\\site-packages\\pyogrio\\raw.py:194: RuntimeWarning: driver GeoJSON does not support open option DRIVER\n",
      "  result = ogr_read(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pft_class</th>\n",
       "      <th>leaf_off_f</th>\n",
       "      <th>urban_prop</th>\n",
       "      <th>agbd</th>\n",
       "      <th>agbd_se</th>\n",
       "      <th>beam</th>\n",
       "      <th>selected_a</th>\n",
       "      <th>sensitivit</th>\n",
       "      <th>solar_elev</th>\n",
       "      <th>rh98</th>\n",
       "      <th>date</th>\n",
       "      <th>lat_cos</th>\n",
       "      <th>lat_sin</th>\n",
       "      <th>lon_cos</th>\n",
       "      <th>lon_sin</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>197.26755</td>\n",
       "      <td>17.124592</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.987684</td>\n",
       "      <td>-22.455900</td>\n",
       "      <td>24.550003</td>\n",
       "      <td>1159</td>\n",
       "      <td>0.982168</td>\n",
       "      <td>0.188008</td>\n",
       "      <td>0.998868</td>\n",
       "      <td>-0.047567</td>\n",
       "      <td>POINT (-2.72642 5.41827)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.55383</td>\n",
       "      <td>17.139278</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.958999</td>\n",
       "      <td>-22.458270</td>\n",
       "      <td>35.879992</td>\n",
       "      <td>1159</td>\n",
       "      <td>0.982138</td>\n",
       "      <td>0.188162</td>\n",
       "      <td>0.998865</td>\n",
       "      <td>-0.047632</td>\n",
       "      <td>POINT (-2.73017 5.42277)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.05653</td>\n",
       "      <td>17.122494</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>-22.455812</td>\n",
       "      <td>21.700007</td>\n",
       "      <td>1159</td>\n",
       "      <td>0.982124</td>\n",
       "      <td>0.188234</td>\n",
       "      <td>0.998866</td>\n",
       "      <td>-0.047606</td>\n",
       "      <td>POINT (-2.72866 5.42488)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210.39091</td>\n",
       "      <td>17.123533</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.980267</td>\n",
       "      <td>-22.444975</td>\n",
       "      <td>26.009990</td>\n",
       "      <td>1159</td>\n",
       "      <td>0.982063</td>\n",
       "      <td>0.188551</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>-0.047490</td>\n",
       "      <td>POINT (-2.72202 5.43412)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.74130</td>\n",
       "      <td>17.122110</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.962343</td>\n",
       "      <td>-22.448800</td>\n",
       "      <td>29.920008</td>\n",
       "      <td>1159</td>\n",
       "      <td>0.982049</td>\n",
       "      <td>0.188624</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>-0.047588</td>\n",
       "      <td>POINT (-2.72763 5.43625)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pft_class  leaf_off_f  urban_prop       agbd    agbd_se  beam  \\\n",
       "997           2           0           0  197.26755  17.124592     8   \n",
       "998           2           0           0  175.55383  17.139278     6   \n",
       "999           2           0           0  100.05653  17.122494     6   \n",
       "1000          2           0           0  210.39091  17.123533     6   \n",
       "1001          2           0           0  175.74130  17.122110     5   \n",
       "\n",
       "      selected_a  sensitivit  solar_elev       rh98  date   lat_cos   lat_sin  \\\n",
       "997            5    0.987684  -22.455900  24.550003  1159  0.982168  0.188008   \n",
       "998            2    0.958999  -22.458270  35.879992  1159  0.982138  0.188162   \n",
       "999            2    0.970559  -22.455812  21.700007  1159  0.982124  0.188234   \n",
       "1000           5    0.980267  -22.444975  26.009990  1159  0.982063  0.188551   \n",
       "1001           2    0.962343  -22.448800  29.920008  1159  0.982049  0.188624   \n",
       "\n",
       "       lon_cos   lon_sin                  geometry  \n",
       "997   0.998868 -0.047567  POINT (-2.72642 5.41827)  \n",
       "998   0.998865 -0.047632  POINT (-2.73017 5.42277)  \n",
       "999   0.998866 -0.047606  POINT (-2.72866 5.42488)  \n",
       "1000  0.998872 -0.047490  POINT (-2.72202 5.43412)  \n",
       "1001  0.998867 -0.047588  POINT (-2.72763 5.43625)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ghana = gpd.read_file(\"GEDI_Ghana.geojson\"\n",
    "                         , driver = 'GeoJSON'\n",
    "                         , engine='pyogrio')\n",
    "\n",
    "df_ghana = df_ghana[df_ghana['pft_class'] == 2]\n",
    "\n",
    "cols_to_keep = ['pft_class', 'leaf_off_f','urban_prop','agbd','agbd_se', 'beam', 'selected_a', 'sensitivit'\\\n",
    "                     , 'solar_elev', 'rh98', 'date', 'lat_cos','lat_sin','lon_cos','lon_sin', 'geometry']\n",
    "\n",
    "df_ghana = df_ghana[cols_to_keep]\n",
    "\n",
    "df_ghana.to_csv(\"df_ghana.csv\", index=False)\n",
    "\n",
    "df_ghana.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "     'pft_class': 'category'\n",
    "    ,'region_cla': 'category'\n",
    "    ,'leaf_off_f': 'category'\n",
    "    ,'urban_prop': 'int64'\n",
    "    ,'agbd': 'float64'\n",
    "    ,'agbd_se': 'float64'\n",
    "    ,'beam': 'category'\n",
    "    ,'elev_lowes': 'float64'\n",
    "    ,'lat_lowest': 'float64'\n",
    "    ,'lon_lowest': 'float64'\n",
    "    ,'selected_a': 'category'\n",
    "    ,'shot_numbe': 'int64'\n",
    "    ,'sensitivit': 'float64'\n",
    "    ,'solar_elev': 'float64'\n",
    "    ,'rh98': 'float64'\n",
    "    ,'pattern': 'object'\n",
    "    ,'doy_sin': 'float64'\n",
    "    ,'doy_cos': 'float64'\n",
    "    ,'date': 'int64'\n",
    "    ,'lat_cos': 'float64'\n",
    "    ,'lat_sin': 'float64'\n",
    "    ,'lon_cos': 'float64'\n",
    "    ,'lon_sin': 'float64'\n",
    "    ,'pft_class_group': 'category'\n",
    "    ,'geometry': 'object'\n",
    "}\n",
    "\n",
    "cols_to_keep = ['pft_class', 'leaf_off_f','urban_prop','agbd','agbd_se', 'beam', 'selected_a', 'sensitivit'\\\n",
    "                     , 'solar_elev', 'rh98', 'date', 'lat_cos','lat_sin','lon_cos','lon_sin', 'geometry']\n",
    "\n",
    "dtypes = {key:value for key, value in dtypes.items() if key in cols_to_keep}\n",
    "\n",
    "df_ghana = df_ghana.astype(dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `bandit` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bandit:\n",
    "    \"\"\"\n",
    "    Bandit to select data for optimizing the model f.\n",
    "\n",
    "    INPUTS:\n",
    "    dataset: includes columns x and y\n",
    "    x: column name of independent variable\n",
    "    y: column name of dependent variable\n",
    "    features: along which to cluster df, including n_bins if numeric\n",
    "    T: number of train points to sample before terminating (must be < frac_train * len(dataset))\n",
    "    frac_train: fraction of dataset for training\n",
    "    frac_test: fraction of dataset for testing\n",
    "    frac_val: fraction of dataset for validation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: pd.DataFrame, x: str, y: str, features: dict, T: int=1000, batch_size: float=10\\\n",
    "                 , frac_train: float=0.48, frac_test: float=0.5, frac_val: float=0.02):\n",
    "\n",
    "        self.dataset        = dataset\n",
    "        self.x              = x\n",
    "        self.y              = y\n",
    "        self.T              = T\n",
    "        self.batch_size     = batch_size\n",
    "        self.frac_train     = frac_train\n",
    "        self.frac_test      = frac_test\n",
    "        self.frac_val       = frac_val\n",
    "        self.train_indices  = []\n",
    "        self.test_indices   = []\n",
    "        self.val_indices    = []\n",
    "        self.used_indices   = []\n",
    "        self.clusters       = []\n",
    "        self.model          = LinearRegression()\n",
    "        self.prev_loss      = np.infty\n",
    "        self.current_loss   = np.infty\n",
    "        self.losses         = []\n",
    "        self.rewards        = []\n",
    "        self.sampled_C      = []     \n",
    "        \n",
    "        self.clean_clusters()\n",
    "        self.generate_clusters(features)\n",
    "        self.ttv_split()\n",
    "\n",
    "        self.num_clusters   = len(self.clusters)\n",
    "        self.alphas         = np.ones(self.num_clusters)\n",
    "        self.betas          = np.ones(self.num_clusters)\n",
    "        self.df_val         = self.dataset[self.dataset.index.isin(self.val_indices)]\n",
    "\n",
    "    def clean_clusters(self):\n",
    "        \"\"\"\n",
    "        Upon initialization, delete all previous cluster assignments.\n",
    "        \"\"\"\n",
    "        cluster_cols = [c for c in self.dataset.columns if 'cluster_ID_' in c]\n",
    "        self.dataset.drop(columns=cluster_cols, inplace=True)\n",
    "\n",
    "    def generate_cluster(self, feature: str, n_bins: int=10):\n",
    "        \"\"\"\n",
    "        Partitions the data into clusters along the specified metadata variable.\n",
    "        If the feature is categorical, then the categories determine the clusters.\n",
    "        If the feature is numeric, then the feature is quantized into n_bins clusters.\n",
    "\n",
    "        INPUTS:\n",
    "        feature: name of feature in dataset along which to define clusters.\n",
    "        n_bins: number of bins, if feature is numeric.\n",
    "\n",
    "        OUTPUTS:\n",
    "        Adds column to self.dataset indicating the cluster ID of an observation along the given metadata feature.\n",
    "        \"\"\"\n",
    "        # get column\n",
    "        try: cluster_column = self.dataset[f'{feature}']\n",
    "        except KeyError: raise KeyError(f\"Column {feature} does not exist in dataset\")\n",
    "\n",
    "        # numeric\n",
    "        if ptypes.is_numeric_dtype(cluster_column):\n",
    "\n",
    "            # -1 for bins\n",
    "            cluster_ids = pd.qcut(cluster_column, q=n_bins-1, labels=False, duplicates='drop')\n",
    "\n",
    "        # categorical\n",
    "        elif ptypes.is_categorical_dtype(cluster_column):\n",
    "            cluster_ids = cluster_column.cat.codes\n",
    "\n",
    "        # count\n",
    "        n_cluster = len(cluster_ids.unique())\n",
    "\n",
    "        # save IDs\n",
    "        feature_name = f\"cluster_ID_{feature}\"\n",
    "        self.dataset[feature_name] = cluster_ids\n",
    "\n",
    "        # save clusters\n",
    "        for k in range(n_cluster+1):\n",
    "            self.clusters.append((feature_name, k)) \n",
    "            \n",
    "    def generate_clusters(self, features: dict):\n",
    "        \"\"\"\n",
    "        Partitions the data into clusters along the specified metadata variables.\n",
    "        \n",
    "        INPUTS:\n",
    "        features: dictionary of form {feature: n_bins}. The n_bins argument is ignored when feature is categorical.\n",
    "        \"\"\"\n",
    "        for feature, n_bins in features.items():\n",
    "            self.generate_cluster(feature, n_bins)\n",
    "        \n",
    "    def ttv_split(self):\n",
    "        \"\"\"\n",
    "        Split dataset into train, test and validation components.\n",
    "        \"\"\"\n",
    "        data_size = len(self.dataset.index)\n",
    "        assert np.isclose(self.frac_train + self.frac_test + self.frac_val, 1.0), \"Train, test, validation fractions must sum to 1\"\n",
    "\n",
    "        pos_train   = int(data_size * self.frac_train)\n",
    "        pos_test    = int(data_size * self.frac_test)\n",
    "\n",
    "        split_indices = [pos_train, pos_train + pos_test]\n",
    "\n",
    "        train_indices, test_indices, val_indices = np.split(self.dataset.index, split_indices)\n",
    "\n",
    "        self.train_indices  = train_indices\n",
    "        self.test_indices   = test_indices\n",
    "        self.val_indices    = val_indices\n",
    "\n",
    "    def sample_reward_probabilities(self):\n",
    "        \"\"\"\n",
    "        Sample reward probabilities from multivariate Beta distribution.\n",
    "        \"\"\"\n",
    "        pi = np.array([np.random.beta(a, b) for a, b in zip(self.alphas, self.betas)]).T\n",
    "        return pi\n",
    "\n",
    "    def sample_datapoint(self, pi):\n",
    "        \"\"\"\n",
    "        Sample a datapoint and add to the training dataset. Return cluster sampled.\n",
    "\n",
    "        INPUTS:\n",
    "        pi: reward probabilities\n",
    "        \"\"\"\n",
    "        # find first non-empty cluster from which to sample\n",
    "        pi_descending = np.argsort(pi)[::-1]\n",
    "        counter = 0\n",
    "\n",
    "        # runs at least once and until a non-empty cluster is found\n",
    "        while counter == 0 or cluster.empty:\n",
    "            j = pi_descending[counter]\n",
    "            feature, value = self.clusters[j]\n",
    "            cluster = self.dataset[  (self.dataset[feature] == value) \n",
    "                                   & (self.dataset.index.isin(self.train_indices)) \n",
    "                                   & (~self.dataset.index.isin(self.used_indices))\n",
    "                                   ]\n",
    "            counter += 1\n",
    "            \n",
    "        # pick datapoint\n",
    "        cluster_size = len(cluster.index)\n",
    "        s_index = cluster.index[np.random.randint(0, cluster_size)]\n",
    "        self.used_indices.append(s_index)\n",
    "        self.sampled_C.append(j)\n",
    "        return j\n",
    "    \n",
    "    def score_prediction(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Compute loss for batch of predicitons.\n",
    "\n",
    "        INPUTS:\n",
    "        y: true values\n",
    "        y_hat: predicted values\n",
    "        \"\"\"\n",
    "        return mean_absolute_percentage_error(y, y_hat)\n",
    "\n",
    "    def compute_reward(self):\n",
    "        \"\"\"\n",
    "        Compute reward with current train indices.\n",
    "        \"\"\"\n",
    "        X = self.dataset[self.dataset.index.isin(self.used_indices)][self.x].to_numpy()\n",
    "        y = self.dataset[self.dataset.index.isin(self.used_indices)][self.y].to_numpy()\n",
    "\n",
    "        # reshape for single feature\n",
    "        X = X.reshape(-1, 1)\n",
    "\n",
    "        self.model.fit(X=X, y=y)\n",
    "\n",
    "        X_val   = self.df_val[self.x].to_numpy()\n",
    "        y_val   = self.df_val[self.y].to_numpy()\n",
    "        \n",
    "        X_val   = X_val.reshape(-1,1)\n",
    "\n",
    "        y_hat   = self.model.predict(X_val)\n",
    "\n",
    "        self.prev_loss      = self.current_loss\n",
    "        self.current_loss   = self.score_prediction(y=y_val, y_hat=y_hat)\n",
    "\n",
    "        reward = 1 if self.current_loss < self.prev_loss else 0\n",
    "\n",
    "        self.rewards.append(reward)\n",
    "        self.losses.append(self.current_loss)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def update_beta_params(self, reward: int, j: int):\n",
    "        \"\"\"\n",
    "        Update parameters for cluster sampling distributions based on reward.\n",
    "\n",
    "        INPUTS:\n",
    "        reward: reward from latest datapoint selection\n",
    "        j: index of cluster from which datapoint selected\n",
    "        \"\"\"\n",
    "        if reward == 1: self.alphas[j] += 1\n",
    "        else:           self.betas[j] += 1\n",
    "\n",
    "    def run_MABS(self):\n",
    "        \"\"\"\n",
    "        Run the multi-armed bandit selection algorithm.\n",
    "        \"\"\"\n",
    "        t = 1\n",
    "\n",
    "        # run for T time steps\n",
    "        while t <= self.T:\n",
    "\n",
    "            # run for batch_size obs before computing reward\n",
    "            for _ in range(self.batch_size):\n",
    "                pi = self.sample_reward_probabilities()\n",
    "                j = self.sample_datapoint(pi)\n",
    "\n",
    "            r = self.compute_reward()\n",
    "            self.update_beta_params(reward=r, j=j)\n",
    "\n",
    "            t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {'beam':None, 'sensitivit':10, 'solar_elev':10}\n",
    "bandit_boi = bandit(df_ghana, x='rh98', y='agbd', features=features, T=1000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_boi.run_MABS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bandit_boi.pkl', 'rb') as file:\n",
    "    bandit_boi = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cluster_ID_beam', 0),\n",
       " ('cluster_ID_beam', 1),\n",
       " ('cluster_ID_beam', 2),\n",
       " ('cluster_ID_beam', 3),\n",
       " ('cluster_ID_beam', 4),\n",
       " ('cluster_ID_sensitivit', 0),\n",
       " ('cluster_ID_sensitivit', 1),\n",
       " ('cluster_ID_sensitivit', 2),\n",
       " ('cluster_ID_sensitivit', 3),\n",
       " ('cluster_ID_sensitivit', 4),\n",
       " ('cluster_ID_sensitivit', 5),\n",
       " ('cluster_ID_sensitivit', 6),\n",
       " ('cluster_ID_sensitivit', 7),\n",
       " ('cluster_ID_sensitivit', 8),\n",
       " ('cluster_ID_sensitivit', 9),\n",
       " ('cluster_ID_solar_elev', 0),\n",
       " ('cluster_ID_solar_elev', 1),\n",
       " ('cluster_ID_solar_elev', 2),\n",
       " ('cluster_ID_solar_elev', 3),\n",
       " ('cluster_ID_solar_elev', 4),\n",
       " ('cluster_ID_solar_elev', 5),\n",
       " ('cluster_ID_solar_elev', 6),\n",
       " ('cluster_ID_solar_elev', 7),\n",
       " ('cluster_ID_solar_elev', 8),\n",
       " ('cluster_ID_solar_elev', 9)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_boi.clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
